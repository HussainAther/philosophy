# The brain-computer interface

Brains are only like computers in a specific abstract sense. We can take apart this analogy in the context
of the brain-computer interface to determine knowledge for philosophy, neuroscience, artificial intelligence,
and other research areas. It's very harmful in many ways to treat the nervous system as the hardware in such 
a way that we need to understand the cognitive science as software when we don't understand the limitations 
of such a metaphor. Any theory of anatomical connection we demonstrate in vertebrate nervous systems may give
us a basic description of what happens at each stage, but don't tell us how a given input relates to a certain
output. Instead, they obfuscate the description of the brain by using unneccessary comparisons to explain
phenomena that are better off explained by describing the phenomena directly and precisely.

An output of a computer depends on its program, input, and functional stages that lead to the output.

## Intentionality 

If we treat propositional attitudes with intentionality as a physical properties, we can build a computer
with states that have genuine intentionality. But no computer model that stimulates human propositional
attitudes will have genuine intentional states. Intentionality of propositional attitudes isn't a physical
property. 

We may consider the network theory of meaning (or holistic theory or conceptual-role theory) such that the meaning
of an expression plays a role in its internal representatinoal economy. This way it relates to sensory input and
behavioral output. Meaning is relational as an expression's meaning is a function of its inferential and computational
role in a person's internal system. A robot that behaves like a human is still subject to the quesitno of whether
those thoughts it generates have the same meaning that represent our own meaning. Assigning meaning to the internal
states of a robot would be applying a double standard arbitrarily with no useful purpose. The robot's internal 
machinery doesn't change that it believes, wants, and understands things. The robot's intentional states depend
on how complex its internal informational network of states it has.

We need altogether a better theory of representation in organisms much the same way we have theoretical definitions
and ideas of what molecules, proteins, and neutrons are. 

## Levels of organization

The brain-computer analogy presents a problem of complexity that we know we have in the brain as that relates to
organization of a computer. The semantic, syntactic, and mechanistic levels introduce issues with the level of the
algorithm and the structural implementation of those features. Neurobiological theory challenges the way of
specifiying the organizational description. The levels of membrane, cell, synapse, cell assembly, circuit, and behavior
can be argued as levels, but even within them we have different partitions of the levels of themselves. We can also determine
levels by the research methods such as how through learning and memory we can take a cellular approach to show 
modifications in presynaptic neurotransmitter releases in habituation. Which level is functional and which level is structural
is difficult to determine, too. 

## Causality

A calculator's representation and rules for manipulating representations can explain its behavior much the same way
we describe how and why people do what they do. Philosopher Zenon Pylyshyn said we explain why a machine does
something with certain interpretations of the symbols in a domain. Psychologcial theory would cross-classify
categories of neurophysiology theory that would make neurophysiological generalizations miss important relations
that are only describably at the level at which representations are referred to. The psychological maps only would 
map onto an indefinite mix of neurobiological categories.

## Connectionism (Parallel distributed processing)

As Paul Churchland has argued, we may use connectionism or parallel distributed processing (PDP) in figuring out the computational
operatinos in nervous systems in such a way we may use computer models of parallel distributed systems to generate the appropriate
phenomena on a higher level (cognitive science, psychology, etc.) from basic processes (neuroscience, physics, etc.).

## Tensor networks

 
