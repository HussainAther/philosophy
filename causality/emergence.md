# Contextual emergence

## What is contextual emergence?

Contextual emergence is a specific kind of relationship between different
domains of scientific descriptions of particular phenomena. Although these domains 
are not ordered strictly hierarchically, one often speaks of lower and higher levels 
of description in which emergence occurs. From the lower levels (L), more fundamental in a certain
sense, phenomena emerge in higher levels (H) in more complex phenomena. Strings of DNA in a genome
may correspond to different transcripts on an transcriptome level for an individual. Chaotic conditions
may emerge from certain differential equations subject to certain constraints. This complexity depends
on the conditions of the context. Hence, contextual emergence.

Contextual emergence involves  well-defined relationships between different levels of complexity.
We can use a two-step procedure to create a systematic, formal way that an individual
description (Li) creates a statistical description (Ls) among the lower level. This process
can lead us to decribe individuals at a higher level (Hi). We iterate this process (Li -> Ls -> Hi) through
sets of descriptions connected with one another to reveal what emerges at higher levels.

During this method, we identify equivalence classes of individual states that are indistinguishable
with respect to a certain property of the entire system. We can realize different statistical
states in Ls by individual states in Li. Each state has limited knowledge, but, together,
we can create probability distributions represent the statistical states Ls. This could be how
spike signals from neural circuits encode for higher-level functions in the brain.

Then, we can assign individual states at the higher level H to coextensional statistical states at level L.
We use a top-down constraint. This needs information about the higher description to choose a
context setting the framework for the set of observable properties at level H created from L.
We can implement stability criteria at level L such that the appropriate context emerges at level H.
The stability refers to the ability for the features of the system to remain valid even under
small changes. This includes equilibrium states of gas systems and homeostatic relationships
between units of biological mechanisms such as glycolysis. We may also define stability as systems
that have boundaries maintained under the dynamics specified for it We may choose to confine ourselves
to certain electrochemical properties that emerge from membrane dynamics in synaptic networks. This allows
the emergent properties to remain well-defined from the contextual topology of L. It also
tells us which properties of L are relevant to the contextual emergence of H.

This interplay between upward and downard strategies lets the system remain self-consistent.
moving from a higher context to a lower one requires the stability conditions to lead to lower-level
partitions of the system while moving to a higher context means the statistics of lower-level states
extend to higher-level individual states we can observe.

## Point mechanics to statistical mechanics to thermodynamics

We can even demonstrate the relationship between different fields of science through contextual
emergence. Moving from classical point mechanics, involving forces due to gravitational effects and
electromagnetism, to statistical mechanics to thermodynamics illustrates this phenomena. From point mechanics
to statistical mechanics particles or other individual units (Li) form ensemble distributions which can be
studied using statistics. We can define many-particle systems with statistical ensemble descriptions (Ls)
like momenta or energy of distributions, such as the Maxwell-Boltzmann distribution for N particles. From there,
we can find mean kinetic energy, Gibbs free energy, entropy, and other statistical quantities.

We can observe expectation values of momenta distrubitioins of particle ensembles to calculate temperature
of the system as a higher-level function (Hi) on the assumption the system is in equilibrium. The zeroth law
of thermodynamics does not come from statistical mechnics, but from thermodynamics. Other features such as
irreversibility and adiabatic nature emerge as well. We can characterize this thermal equilibrium (Hi)
using Kubo-Martin-Schwinger (KMS) states, defined by the condition that characterizes the structural
stability of a KMS state against local perturbations or changes. This leads to stationarity, ergodicity,
and mixing using the zeroth law of thermodynamics to define the system as stable. We can
also use the second law of thermodynamics to express the stability in maximization of entropy
for thermal equilibrium states.

The first step of the contextual emergence process (Li -> Ls) describes statistical states from the individual
states, and the second gives indiidual thermal states from statistical mechanical states. Other examples may
include emergence of geometric optics from electrodynaimcs, electrical engineering features from electrodynamics,
chirality from quantum mechanics, and diffusion or friction of a quantum particle in a thermal medium. Neuroscientists
have even found use in contextually emerging cognitive states from nerual correlates.

## Hodgkin-Huxley equations

The Hodgkin-Huxley equations that describe generation and propagation of action potentisl form a system
fo four ordinary nonlinear differential equations: an electric conductance equation for transmemberane
currents and three master equations for the opening kinetics of sodium and potassium channels. These
lower-level stochastic (using Markov processes as transition probabilities) phenomena lead to higher-level
descriptions of ion channel function to characterize a deterministic dynamic system. We can treat ion channels
as macro-molecular quantum objects with the Schrödinger equation for many particles. The Schrödinger equation
describes a highly entangled state of electrons and atomic nuclei as a whole, and, on a moleculr level,
the structure of a closed or open pore of an ion channel through the Born-Oppenheimer approximation
separates electronic and nucleonic wave functions. Then, we can use the electronic quantum dynamics
in a constrainted rigid nucleonic frame that has a classical spatial structure. This stoachstic spatial structure
gives the equations of the Hodgkin-Huxley system as a contextually emergent phenomenon.

## Mental states emerging from neuroscience

To realize mental states from neural states, we specify the L level as neuron states of neural assemblies
in the brain with respect ot H, a class of mental states that reflects the situation under study. We may use
experimental protocols that include a task for subjects to define mental states while recording brain states.
We may use individual neuron properties Li to find Ls such taht statisical states have equivalence classes
of those individual states. The differences must be irrelevant with respect to the higher level H. Philosopher
David Chalmers said a neural correlate of a conscious mental state can be mulltiply realized by "minimally sufficient
neural subsystems correlated with states of consciousness" in "What is a neural correlate of consciousness?"
We can look at phenomenal families, sets of mutualyl exclusive phenomenal mental states that jointly
partition a space of mental states. Creature consciousness can give us refined levels of phenomenal states
of background consciousness (awake, dreaming, etc.), wake consciosuness (perceptual, cognitive, affective, etc.),
perceptual consciousness (visual, audotiry, tactile, etc.), and visual consicousness (color, form, location, etc.). With
one of these contexts, we choose stability criterion at Ls that has complicated neurodynamics to find robust, proper
 statistical states.
 
 We may describe L-dynamics and H-dynamics meshing with one another if coarse graining and time
 evolution commute with one another. We create meshes, parts of space differentiated by complexes of cells
 between the two levels, that follow from higher-level stability criterion. The coarse graining means fine details
 of the system can be smoothed over, as entropy of the system increases, such that we can make predictions
 about the system as a whole.

We can represent proper cells with basins of attraction and chaotic attractors with coarse-grained generating
partitions. These partitions of the system lead to Markov chains with a rigorous theoretical constraint
for the propert definition of stable mental states. The mathematical techniques come from ergodic theory and
symbolic dynamics.

The emergence of metnal states from electroencephalogram (EEG) dynamics shows
that data from subjects with EEG data from sporadic epileptic seizures can correlate with mental states
of the seizures themselves. Using a 20-channel EEG recording, we get a 20-dimension state space
that we reduce to a lower number through principal component restrictions. We find a homogeneous grid
of cells to set up a Markov transition matrix that reflects the EEG dynamics using fine-graned auxilary
partition. Then, this matrix gives eigenvalues that characterize time scales for which the dynamics can
be ordered by size. The eigenvectors span an eigenvector space such that the measure principal component
states form a simplex. The three leading eigenvalue give a neural state repesentation that has a 2-simplex
with three vertices, or a triangle. We can further classify neural states by distance frmo the vertices
of the simplex to clusters of neural data. In the principal component state space, the clsuters appear
as non-intersecting convex sets between mental states. We may alos use recurrence
structure analysis to partition the state space into recurrent clusters such that they overlap from the
recurrence plot of the dnyamical system. We figure out the metastable states and transitions
between them using a Markov chain with one distinguished transient state and other states representing
the metastable states in the dynamics.

## Intentionality

Philosopher Daniel Dennett describes the intentional stance of the prediction of an system's behavior
too complex to be treated as either a physical or desgined system. Intentional systems behave in predicted
ways by ascribing beliefs and desires to their internal states. From thermostats to chess computers,
we can make predictions of a system with necessary and sufficient coonditions. The system's dynamics have
to be non-trivial, so this excludes linear systems with periodic oscillations or damped relaxations. We construct
an intentional hierarchy from general case of nonlinear nonequilibrium dissipative systems
to more specific intentional systems. A physical system's physical nature is necessary for being a nonlinear
dissipative nonequilibrium system while a nonlinear dissipative nonequiliibrium nature is necessary for
an intentional system. An intentional system is necessary for being a true believer, according to Dennett.
Sufficient conditions in the intentinoal hierarchy implement contextual stability conditions.

The transition from equilibrium thermodynamcis to fluid dynamics represents phenomenal laws of fluid
dynamics (like the Navier-Stokes equation) emerging from statistical mechancis under the assumption of
local equilibrium. Sufficient boundary conditions give rise to self-organization, such as through "magnetic snakes."
We give a rationality constraint for optimal dissipation of pumped energy, and true believers emerge contextually
as intentional systems under mutual adoption of the intentional stance.

## Symbolic grounding

The symbolic grounding problem is the problem fo assigning meaning to symbols on purely syntactic grounds.
Cognitivists such as philosophers Jerry Fodor and Zenon Pylyshyn have described this problem. It can also describe
how the question of how conscious mental states can be characterized by neural correlates. The relation between analog
and digital systems such that syntactic digital symbols relate to the analog behavior of a system they descibe
symbolically needs to be further examined through dnyamical automata. Piecewise linear time-discrete
maps over a two-dimensional state space assume the interpretation as symbolic computers through a rectangular
partition of the unit square. A single point trajectory is not fully interpretable as symbolic computation.
We need higher-level macrostates from ensembles of state space points, or probability distributions of points,
taht evolve under the dynamcis.



Beim Graben and Potthast showed that only uniform probability distributions with rectangular support 
exhibit a stable dynamics that is interpretable as computation. Thus, the huge space of possible 
probability distributions must be contextually restricted to the subclass of uniform probability 
distributions in order to obtain meaningfully grounded symbolic processes. In this sense, symbol 
grounding is contextually emergent.

It is a long-standing philosophical puzzle how the mind can be causally relevant in a physical world: 
the problem of mental causation the question of how mental phenomena can be causes is of high significance 
for an adequate comprehension of scientific disciplines such as psychology and cognitive neuroscience. 
Moreover, mental causation is crucial for our everyday understanding of what it means to be an agent 
in a natural and social environment. Without the causal efficacy of mental states the notion of agency 
would be nonsensical.

One of the reasons why the causal efficacy of the mental has appeared questionable is that a 
horizontal (intralevel, diachronic) determination of a mental state by prior mental states 
seems to be inconsistent with a vertical (interlevel, synchronic) determination of that mental 
state by neural states. In a series of influential papers and books, Kim has presented his much 
discussed supervenience argument (also known as exclusion argument), which ultimately amounts to 
the dilemma that mental states either are causally inefficacious or they hold the threat of 
overdetermining neural states. In other words: either mental events play no horizontally 
determining causal role at all, or they are causes of the neural bases of their relevant 
horizontal mental effects. 

The interlevel relation of contextual emergence yields a quite different perspective on mental 
causation. It dissolves the alleged conflict between horizontal and vertical determination of mental 
events as ill-conceived. The key point is a construction of properly defined mental states from the 
dynamics of an underlying neural system. This can be done via statistical neural states based on a 
proper partition, such that these statistical neural states are coextensive (but not necessarily 
identical) with individual mental states.

This construction implies that the mental dynamics and the neural dynamics, related to each other 
by a so-called intertwiner, are topologically equivalent. Given properly defined mental states, the 
neural dynamics gives rise to a mental dynamics that is independent of those neurodynamical details 
that are irrelevant for a proper construction of mental states.

As a consequence, (i) mental states can indeed be causally and horizontally related to other mental states, 
and (ii) they are neither causally related to their vertical neural determiners nor to the neural determiners 
of their horizontal effects. This makes a strong case against a conflict between a horizontal and a vertical 
determination of mental events and resolves the problem of mental causation in a deflationary manner. 
Vertical and horizontal determination do not compete, but complement one another in a cooperative fashion. 
Both together deflate Kim's dilemma and reflate the causal efficacy of mental states. Our conclusions 
match with and refine the notion of proportionate causation introduced by Yablo.

In this picture, mental causation is a horizontal relation between previous and subsequent mental states, 
although its efficacy is actually derived from a vertical relation: the downward confinement of 
(lower-level) neural states originating from (higher-level) mental constraints. This vertical 
relation is characterized by an intertwiner, a mathematical mapping, which must be distinguished 
from a causal before-after relation. For this reason, the terms downward causation or top-down causation 
are infelicitous choices for addressing a downward confinement by contextual constraints.

Within the tradition of dual-aspect thinking, one can distinguish two different, in a sense opposing 
base conceptions. In one of them, psychophysically neutral elementary entities are composed to sets 
of such entities, and depending on the composition these sets acquire mental or physical properties. 
The other base conception refers to a psychophysically neutral domain which does not consist of 
elementary entities waiting to be composed, but is conceived as one overarching whole that is to 
be decomposed. In contrast to the atomistic picture of compositional dual-aspect monism, the holistic 
picture of the decompositional variant is strongly reminiscent of the fundamental insight of 
entanglement in quantum physics.

The contextual emergence of both the mental and the material from a psychophysically neutral 
whole requires a fresh look at the conceptual framework, both technically and in terms of the 
underlying metaphysics. At the technical level, we do now refer to the contextual emergence of 
multiplicity from unity, fine grains from coarse grains, rather than the other way around. The basic 
idea here is that a "primordial" decomposition of an undivided whole generates (under a particular 
context) different domains that give rise to differentiations, e.g. the mind-matter distinction.

In the decompositional variety of dual-aspect monism, refinement by symmetry breakdown is conceptually 
prior to its opposite of generalization, where the restoration of symmetries generates equivalence 
classes of increasing size. The basic undivided, psychophysically neutral reality is the trivial 
partition where nothing is distinguished. There is full symmetry and, hence, the corresponding 
reality is "ineffable", or "discursively inaccessible". Successive decompositions give rise to 
more and more refined partitions, where symmetries are broken and equivalence classes beome 
smaller and smaller. Phenomenal families of mental states illustrate this for the mental domain.

At the metaphysical level, the mental and the physical remain epistemic, but the undivided whole 
is added as an ontic dimension. This reminds one of Plato's ideas or Kant's things-in-themselves, 
which are empirically inaccessible in principle and, in this sense, scientificallly mute. Indeed, 
an undivided whole cannot be further characterized without introducing distinctions that break up 
the wholeness. Yet, it provides one asset in the metaphyscis of the mind-matter problem that no 
other philosophical position provides: the emergence of mind-matter correlations as a direct and 
immediate consequence.

Determinism is often understood as a feature of ontic descriptions of states and observables 
whereas stochasticity refers to epistemic descriptions . Mathematical models 
of classical point mechanics are most common examples of deterministic descriptions, and three 
properties of these descriptions are particularly important: (1) differential 
dynamics, (2) unique evolution, and (3) value determinateness. (1) means essentially that the 
system's evolution obeys a differential equation (or some similar algorithm) in a space of 
ontic states. (2) says that for given initial and boundary conditions there is a unique 
trajectory. (3) assumes that any state be described with arbitrarily small (non-zero) error.

These three points are not independent from each other but define a hierarchy for the contextual 
emergence of deterministic descriptions assuming (1) as a necessary condition for determinism, 
(2) can be proven under the sufficient condition that the trajectories created by a vector field 
obeying (1) pass through points whose distance is stable under small perturbations. Assuming (2) 
for almost every initial condition as a necessary condition of determinism defines a phase flow 
with weak causality. In order to prove (3) one needs strong causality as a sufficient condition.

For a weakly causal system violating (3), trajectories may exponentially diverge, as in chaotic systems. 
In this situation, dilation techniques can lead to contextually emergent
stochasticity in two steps. In the first step, a coarse-graining yields a Markov process. If this 
process is mixing such that it approaches an equilibrium distribution, the deterministic 
dynamics is a Kolmogorov-flow, thereby implementing microscopic chaos as a stability condition.
 
Interestingly, the converse is also possible. For a continuous stochastic process which 
fulfills the Markov criterion, the master equation approach leads to a deterministic "mean-field equation". 
Bishop and beim Graben showed that this situation is analogous to the paradigmatic example of the 
contextual emergence of thermal equilibrium states where thermal KMS macrostates are almost pure, and 
hence almost dispersion-free.

Reproducibility is one of the pillars of scientific methodology, yet it becomes particularly 
difficult in interdisciplinary research where the results to be reproduced typically refer to 
more than one single level of description of the system considered. In such cases it is mandatory 
to distinguish the relevant attributes or observables of the system, depending on its description. 
Usually, different descriptive levels go along with different degrees of granularity. While lower-level 
descriptions address systems in terms of micro-properties (position, momentum, etc.), other, more global, 
macro-properties are more suitably taken into account for higher-level descriptions.

This observation led van Fraassen to the notion of explanatory relativity, where explanations
are not only relationships between theories and facts; they are three-place relations between theories, 
facts, and contexts. The relevance of an explanation is determined by contexts that have to be selected, 
and are not themselves part of a scientific description.

Explanatory relativity backed up by relevance criteria can vitally serve the discussion of reproducibility 
across scientific disciplines. Features that are relevant for a proper explanation of some 
observation should have a high potential to be also relevant for the robust reproduction 
of that observation. But which properties of systems and their descriptions may be promising 
candidates for the application of such relevance criteria? One option to highlight relevance 
criteria is to consider the "granularity" (coarseness) of a description, which usually changes across disciplines.

The transformation between descriptive levels and their associated granularities is possible 
by the interlevel relation of contextual emergence. It yields a formally sound and empirically 
applicable procedure to construct level-specific criteria for relevant observables across disciplines. 
Relevance criteria merged with contextual emergence challenge the old idea of one fundamental 
ontology from which everything else derives. At the same time, the scheme of contextual emergence 
is specific enough to resist the backlash into a relativist patchwork of unconnected model fragments.

Contextual emergence has been originally conceived as a relation between levels of descriptions, 
not levels of nature: It addresses questions of epistemology rather than ontology. In agreement with Esfeld, 
who advocated that ontology needs to regain more significance in science, it would be desirable to 
know how ontological considerations might be added to the picture that contextual emergence provides.

A network of descriptive levels of varying degrees of granularity raises the question of whether 
descriptions with finer grains are more fundamental than those with coarser grains. The 
majority of scientists and philosophers of science in the past tended to answer this question 
affirmatively. As a consequence, there would be one fundamental ontology, preferentially that of 
elementary particle physics, to which the terms at all other descriptive levels can be reduced.

But this reductive credo also produced critical assessments and alternative proposals. A philosophical 
precursor of trends against a fundamental ontology is Quine's ontological relativity. Quine 
argued that if there is one ontology that fulfills a given descriptive theory, then there is more 
than one. It makes no sense to say what the objects of a theory are, beyond saying how to interpret or 
reinterpret that theory in another theory. Putnam later developed a related kind of 
ontological relativity, first called internal realism, later sometimes modified to pragmatic realism.

On the basis of these philosophical approaches, Atmanspacher and Kronz suggested how to apply Quine's 
ideas to concrete scientific descriptions, their relationships with one another, and with their referents. 
One and the same descriptive framework can be construed as either ontic or epistemic, depending on 
which other framework it is related to: bricks and tables will be regarded as ontic by an architect, 
but they will be considered highly epistemic from the perspective of a solid-state physicist.

Coupled with the implementation of relevance criteria due to contextual emergence, the relativity of 
ontology must not be confused with dropping ontology altogether. The "tyranny of relativism" (as 
some have called it) can be avoided by identifying relevance criteria to distinguish proper 
context-specific descriptions from less proper ones. The resulting picture is more subtle and 
more flexible than an overly bold reductive fundamentalism, and yet it is more restrictive and 
specific than a patchwork of arbitrarily connected model fragments.
