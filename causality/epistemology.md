# The epistemology of causality

There are two epistemic approaches to causal theory. Under a hypothetico-deductive account, we hypothesize
causal relationships and deduce predictions based on them. We test these hypotheses and predictions by comparing
empirical phenomena and other knowledge and information on what actually happens to these theories. We may also
take an inductive approach in which we make a large number of appropriate, justified observations (such as a set of data) 
from which we can induce causal relationships directly from them.  

## Hypothetico-Deductive discovery

The testing phase of this account of discovery and causality uses the views on the nature of causality to determine
whether we support or refute the hypothesis. We search for physical processes underlying the causal relationships
of the hypothesis. We can use statistics and probability to determine which consequences of hypotheses are verified, 
like comparing our data to a distribution such as a Gaussian or Dirichlet one. We can further probe these consequences
on a probabilistic level and show that changing hypothesized causes can predict, determine, or guarantee effects.  

Philosopher Karl Popper advocated this approach for causal explanations of events that consist of natural laws, which
are universal statements about the world. He designated initial conditions, single-case statements, from which we may
deduce outcomes and form predictions of various events. These case initial conditions call for effects that we can 
determine, such as whether a physical system will approach thermodynamic equilibrium or how a population might
evolve under the influence of predators or external forces. Popper delineated the method of hypothesizing laws,
deducing their consequences, and rejecting laws that aren't supported as a cyclical process. This is the covering-law
account of causal explanation.  

## Inductive learning

Philosopher Francis Bacon promoted the inductive account of scientific learning and reasoning. From a very high-number of 
observations of some phenomenon or event with experimental, empirical evidence where it's appropriate, we can compile
a table of positive instances (in which a phenomenon occurs), negative instances (it doesn't occur), and prartial instances 
(it occurs to a certain degree). This gives a multidimensionality to phenomena that characterize causal relationships from
both *a priori* and *a posterior* perspectives.

Inductivist artificial intelilgence (AI) approaches have in common the feature that causal relationships can be determined
from statistical relationships. We assume the Causal Markov condition holds of physical causality and physical probability.
This Causal Markov Condition plays a significant deterministic role in the various features of the model and the events or phenomena
it predicts. A causal net must have the Causal Markov Condition as an assumption or premise. For structural equation models (SEM), Causal
Markov Conditions result from representations of each variable as a function of its direct causes and an associated error 
variable with it. We assume probabilistic independence of each error variable. We then find the class of causal models or
a single best causal model with probabilistic independences that are justified by the Causal Markov Condition. They should be
consistent with independences we can infer from the data, and we might also make further assumptions about the minimality 
(no submodel of the causal model also satisfied the Causal Markov Condition), faithfulness (all independences in the data
are implied via the Causal Markov Condition), linearity (all variables are linear functions of their direct causes and
uncorrelated error variables). We may also define causal sufficiency, whether all common causes of measured variables
are measured, and context generality, every individual or node in the model has causal relations of the population. These
two features let us describe models and methods of scientific reasoning as causal in nature and, from there, we may apply
appropriate causal models such as Bayesian, frequentist, or similar methods of prediction. We may even illustrate a causal
diagram or model elements under various conditions such as those given by independence or constraints on variables. 
This way, in the intercorrelatedness of the graph or model, we can't change the value of a variable without affecting
the way it relates to other variables, but there may conditions in which we construct models that have autonomous 
nodes or variables. The way these features and claims of inductivist AI interact with another is subject to debate by
the underlying assumptions, justification, and methods of reasoning behind these models.   
